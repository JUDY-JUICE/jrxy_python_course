{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**批量提取某标签下所有子孙节点标签的NavigableString**\n",
    "\n",
    "对于单个节点的内容可以利用 .string 或 .text 来获得，若想获得多个节点内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(open('html.html','r',encoding='utf-8'), 'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.ul.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in soup.ul.strings:\n",
    "    print(repr(t))    # repr() 函数将对象转化为供解释器读取的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in soup.ul.stripped_strings:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息提取的方法\n",
    "\n",
    "一：完整解析信息的标记形式，再提取关键信息\n",
    "- 需要标记解析器，例如：bs4库的标签树遍历\n",
    "- 优点：信息解析准确\n",
    "- 缺点：提取过程繁琐，速度慢\n",
    "\n",
    "二：无视标记形式，直接搜索关键信息\n",
    "- 对信息的文本查找函数即可\n",
    "- 优点：提取过程简洁，速度较快\n",
    "- 缺点：提取结果准确性与信息内容相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于bs4库的html内容遍历方法\n",
    "\n",
    "![](spider02.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**上行遍历**\n",
    "+ .parent：返回当前节点的父节点标签\n",
    "+ .parents：返回当前节点所有先辈节点标签的迭代类型，仅用于循环遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adult in soup.a.parents:\n",
    "    print(adult.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下行遍历**\n",
    "+ .contents：返回子节点标签的列表\n",
    "+ .children：返回子节点标签的迭代类型，仅用于循环遍历\n",
    "+ .descendants：返回包含所有子孙节点标签的迭代类型，仅用于循环遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.body.contents)\n",
    "soup.body.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.body.children)\n",
    "for child in soup.body.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.body.descendants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**平行遍历（按照HTML文本顺序）**\n",
    "+ .next_sibling：返回下一个平行节点标签\n",
    "+ .previous_sibling：返回上一个平行节点标签\n",
    "+ .next_siblings：返回后续所有平行节点标签的迭代类型\n",
    "+ .previous_siblings：返回前序所有平行节点标签的迭代类型\n",
    "\n",
    "Notice：实际html标签树的平行标签通常是空白/回车/tab等，因此通常仅使用.next_siblings 和 .previous_siblings循环输出当前节点的兄弟节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://www.baidu.com/')\n",
    "res.encoding='utf-8'\n",
    "soup = BeautifulSoup(res.text,'lxml')\n",
    "\n",
    "for sibling in soup.a.next_siblings:\n",
    "    print(repr(sibling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise：**\n",
    "\n",
    "请大家提取出百度主页的html页面的全部标签名称。返回的结果为列表，不包含None并去除重复元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Answer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://www.baidu.com/')\n",
    "res.encoding='utf-8'\n",
    "soup = BeautifulSoup(res.text,'lxml')\n",
    "tagname=[i.name for i in soup.html.descendants]\n",
    "tagname=list(set(filter(None,tagname)))\n",
    "tagname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正则表达式(regular expression, regex, RE)\n",
    "\n",
    ">正则表达式是一种通用的字符串表达框架，可以用来简洁地表达一组字符串，也可以用来判断某字符串的特征归属。\n",
    "\n",
    "正则表达式在文本处理中十分常用：\n",
    "- 表达文本类型的特征\n",
    "- 查找或替换一组字符串\n",
    "- 匹配字符串的全部或部分\n",
    "\n",
    "**正则表达式的常用操作符**\n",
    "```\n",
    ".    匹配除换行符以外的任意字符\n",
    "[  ] 字符集，对单个字符给出取值范围\n",
    "[^ ] 非字符集，对单个字符给出排除范围\n",
    "^    匹配字符串开头\n",
    "$    匹配字符串结尾\n",
    "\\d   匹配数字，等价于[0‐9]\n",
    "\\w   匹配字母或数字或下划线，等价于[A‐Za‐z0‐9_]\n",
    "\n",
    "```\n",
    "\n",
    "**re库的使用**\n",
    "\n",
    ">Re库是Python的标准库，主要用于字符串匹配。调用方式：import re\n",
    "\n",
    "`regex = re.compile(pattern, flags=0)`\n",
    "\n",
    "`将正则表达式的字符串形式编译成正则表达式对象`\n",
    "```\n",
    "pattern : 正则表达式的字符串或原生字符串表示\n",
    "flags : 正则表达式使用时的控制标记\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于bs4库的html内容查找方法\n",
    "\n",
    "`<>.find_all(name, attrs, recursive, string, **kwargs)`  [官方文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#find-all)\n",
    "\n",
    "`返回一个列表类型，存储查找的结果`\n",
    "```\n",
    "name: 查找所有名字为 name 的标签，name 参数的值可以是字符串，正则表达式，列表或是True。\n",
    "attrs: 对标签属性值的进行检索\n",
    "recursive: 是否对子孙节点全部检索，默认True，若设为False则仅检索子节点\n",
    "string: 对<>…</>中字符串区域的检索\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**检索标签名称**\n",
    "\n",
    "- 传入字符串：查找与字符串完整匹配的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 传入列表：返回与列表中任一元素匹配的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(['a','p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 传入True：匹配任何值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all(True):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 传入正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for tag in soup.find_all(re.compile('t')):  #### 返回所有包含t的标签名\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**检索标签属性**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(attrs={'target':'_blank'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all(target='_blank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(href=re.compile(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(href=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**string参数**\n",
    "\n",
    "通过 string 参数可以搜索文档中的字符串内容。string 参数接受字符串、正则表达式、列表、True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all(string=[re.compile(\"利好\"), re.compile(\"新高\"), re.compile(\"扩大\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a',string=[re.compile(\"利好\"), re.compile(\"新高\"), re.compile(\"扩大\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**limit参数**\n",
    "\n",
    "find_all()方法返回全部的搜索结构，如果文档树很大那么搜索会很慢。如果我们不需要全部结果，可以使用 limit 参数限制返回结果的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(string=[re.compile(\"利好\"), re.compile(\"新高\"), re.compile(\"扩大\")], limit = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**find_all的扩展方法**\n",
    "\n",
    "- <>.find()：搜索且只返回一个结果\n",
    "- <>.find_parents()：在先辈节点中搜索，返回列表类型\n",
    "- <>.find_parent()：在先辈节点中返回一个结果\n",
    "- <>.find_next_siblings()：在后续平行节点中搜索，返回列表类型\n",
    "- <>.find_next_sibling()：在后续平行节点中返回一个结果\n",
    "- <>.find_previous_siblings()：在前序平行节点中搜索，返回列表类型\n",
    "- <>.find_previous_sibling()：在前序平行节点中返回一个结果\n",
    "\n",
    "区别：检索区域和返回结果不同\n",
    "\n",
    "具体请参阅文档 https://beautifulsoup.readthedocs.io/zh_CN/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise：**\n",
    "\n",
    "1. 请提取页面中的股票代票、股票名称、公告日期这三项信息，并将数据存放在dataframe中。\n",
    "2. 请提取出下述url页面中红色的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "## 使用一个网易的数据页面\n",
    "url=\"http://quotes.money.163.com/data/caibao/yjgl_ALL.html?reportdate=20200930&sort=publishdate&order=desc&page=0\"\n",
    "     \n",
    "def request_url(url):\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.109 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent} \n",
    "    \n",
    "    res = requests.get(url,headers=headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    return res.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 基于上述实例，尝试通过修改url = 'https://finance.sina.com.cn/roll/index.d.html?cid=56588&page=1'\n",
    "中的关键词 page，实现自动翻页爬取多条信息，并存入本地数据库中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise：**\n",
    "\n",
    "1. 下面是一个批量下载百度图片的例子，请通过修改num，下载更多的图片，将num作为循环变量的参数写入程序中。\n",
    "2. 当下载的图片很多时会发生各种错误，如何让程序稳定地运行？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "pathname = 'D:\\\\图片\\\\'\n",
    "if not os.path.exists(pathname):\n",
    "    os.makedirs(pathname)\n",
    "name = input('您要爬取的图片名称：')\n",
    "num = 1\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'}\n",
    "url = 'https://image.baidu.com/search/flip?tn=baiduimage&ie=utf-8&word='+name+'&pn='+str((num-1)*60)\n",
    "res = requests.get(url,headers=headers)\n",
    "# print(res.request.url)\n",
    "\n",
    "html = res.content.decode()\n",
    "addlist = re.findall('\"objURL\":\"(.*?)\",',html)\n",
    "# print(len(addlist))\n",
    "# print(addlist)\n",
    "\n",
    "j=0\n",
    "for i in addlist:\n",
    "    j = j +1\n",
    "    img = requests.get(i)\n",
    "    f = open(pathname+name+str(j)+'.jpg','ab')\n",
    "    print('---------正在下载第'+str(j)+'张图片----------')\n",
    "    f.write(img.content)\n",
    "    f.close()\n",
    "print('下载完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**延申：**\n",
    "\n",
    "1. 在之前的课程中，我们提到过另一种解析器lxml，它提供另一种方法XPath来提取网页中的数据，感兴趣的同学可参阅[click](https://www.cnblogs.com/cathycheng/p/11422685.html)\n",
    "\n",
    "2. 思考如何爬取动态网页。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结：\n",
    "\n",
    "1. 提取NavigableString的几种方法：text、strings、stripped_strings\n",
    "\n",
    "2. 遍历网页标签树：上行，下行，平行\n",
    "\n",
    "3. 利用find_all方法搜索标签树：对标签名称、属性、内容的搜索（正则表达式）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
