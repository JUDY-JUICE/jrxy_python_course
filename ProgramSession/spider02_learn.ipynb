{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**批量提取某标签下所有子孙节点标签的NavigableString**\n",
    "\n",
    "对于单个节点的内容可以利用 .string 或 .text 来获得，若想获得多个节点内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(open('html.html','r',encoding='utf-8'), 'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.ul.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in soup.ul.strings:\n",
    "    print(repr(t))    # repr() 函数将对象转化为供解释器读取的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in soup.ul.stripped_strings:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息提取的方法\n",
    "\n",
    "一：完整解析信息的标记形式，再提取关键信息\n",
    "- 需要标记解析器，例如：bs4库的标签树遍历\n",
    "- 优点：信息解析准确\n",
    "- 缺点：提取过程繁琐，速度慢\n",
    "\n",
    "二：无视标记形式，直接搜索关键信息\n",
    "- 对信息的文本查找函数即可\n",
    "- 优点：提取过程简洁，速度较快\n",
    "- 缺点：提取结果准确性与信息内容相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于bs4库的html内容遍历方法\n",
    "\n",
    "![](spider02.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**上行遍历**\n",
    "+ .parent：返回当前节点的父节点标签\n",
    "+ .parents：返回当前节点所有先辈节点标签的迭代类型，仅用于循环遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adult in soup.a.parents:\n",
    "    print(adult.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下行遍历**\n",
    "+ .contents：返回子节点标签的列表\n",
    "+ .children：返回子节点标签的迭代类型，仅用于循环遍历\n",
    "+ .descendants：返回包含所有子孙节点标签的迭代类型，仅用于循环遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.body.contents)\n",
    "soup.body.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.body.children)\n",
    "for child in soup.body.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.body.descendants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**平行遍历（按照HTML文本顺序）**\n",
    "+ .next_sibling：返回下一个平行节点标签\n",
    "+ .previous_sibling：返回上一个平行节点标签\n",
    "+ .next_siblings：返回后续所有平行节点标签的迭代类型\n",
    "+ .previous_siblings：返回前序所有平行节点标签的迭代类型\n",
    "\n",
    "Notice：实际html标签树的平行标签通常是空白/回车/tab等，因此通常仅使用.next_siblings 和 .previous_siblings循环输出当前节点的兄弟节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://www.baidu.com/')\n",
    "res.encoding='utf-8'\n",
    "soup = BeautifulSoup(res.text,'lxml')\n",
    "\n",
    "for sibling in soup.a.next_siblings:\n",
    "    print(repr(sibling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise：**\n",
    "\n",
    "请大家提取出百度主页的html页面的全部标签名称。返回的结果为列表，不包含None并去除重复元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Answer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://www.baidu.com/')\n",
    "res.encoding='utf-8'\n",
    "soup = BeautifulSoup(res.text,'lxml')\n",
    "tagname=[i.name for i in soup.html.descendants]\n",
    "tagname=list(set(filter(None,tagname)))\n",
    "tagname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正则表达式(regular expression, regex, RE)\n",
    "\n",
    ">正则表达式是一种通用的字符串表达框架，可以用来简洁地表达一组字符串，也可以用来判断某字符串的特征归属。\n",
    "\n",
    "正则表达式在文本处理中十分常用：\n",
    "- 表达文本类型的特征\n",
    "- 查找或替换一组字符串\n",
    "- 匹配字符串的全部或部分\n",
    "\n",
    "**正则表达式的常用操作符**\n",
    "```\n",
    ".    匹配除换行符以外的任意字符\n",
    "[  ] 字符集，对单个字符给出取值范围\n",
    "[^ ] 非字符集，对单个字符给出排除范围\n",
    "^    匹配字符串开头\n",
    "$    匹配字符串结尾\n",
    "\\d   匹配数字，等价于[0‐9]\n",
    "\\w   匹配字母或数字或下划线，等价于[A‐Za‐z0‐9_]\n",
    "\n",
    "```\n",
    "\n",
    "**re库的使用**\n",
    "\n",
    ">Re库是Python的标准库，主要用于字符串匹配。调用方式：import re\n",
    "\n",
    "`regex = re.compile(pattern, flags=0)`\n",
    "\n",
    "`将正则表达式的字符串形式编译成正则表达式对象`\n",
    "```\n",
    "pattern : 正则表达式的字符串或原生字符串表示\n",
    "flags : 正则表达式使用时的控制标记\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于bs4库的html内容查找方法\n",
    "\n",
    "`<>.find_all(name, attrs, recursive, string, **kwargs)`  [官方文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#find-all)\n",
    "\n",
    "`返回一个列表类型，存储查找的结果`\n",
    "```\n",
    "name: 查找所有名字为 name 的标签，name 参数的值可以是字符串，正则表达式，列表或是True。\n",
    "attrs: 对标签属性值的进行检索\n",
    "recursive: 是否对子孙节点全部检索，默认True，若设为False则仅检索子节点\n",
    "string: 对<>…</>中字符串区域的检索\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**检索标签名称**\n",
    "\n",
    "- 传入字符串：查找与字符串完整匹配的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 传入列表：返回与列表中任一元素匹配的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(['a','p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 传入True：匹配任何值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all(True):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 传入正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for tag in soup.find_all(re.compile('t')):  #### 返回所有包含t的标签名\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**检索标签属性**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(attrs={'target':'_blank'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all(target='_blank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(href=re.compile(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(href=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**string参数**\n",
    "\n",
    "通过 string 参数可以搜索文档中的字符串内容。string 参数接受字符串、正则表达式、列表、True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all(string=[re.compile(\"利好\"), re.compile(\"新高\"), re.compile(\"扩大\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a',string=[re.compile(\"利好\"), re.compile(\"新高\"), re.compile(\"扩大\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**limit参数**\n",
    "\n",
    "find_all()方法返回全部的搜索结构，如果文档树很大那么搜索会很慢。如果我们不需要全部结果，可以使用 limit 参数限制返回结果的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(string=[re.compile(\"利好\"), re.compile(\"新高\"), re.compile(\"扩大\")], limit = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**find_all的扩展方法**\n",
    "\n",
    "- <>.find()：搜索且只返回一个结果\n",
    "- <>.find_parents()：在先辈节点中搜索，返回列表类型\n",
    "- <>.find_parent()：在先辈节点中返回一个结果\n",
    "- <>.find_next_siblings()：在后续平行节点中搜索，返回列表类型\n",
    "- <>.find_next_sibling()：在后续平行节点中返回一个结果\n",
    "- <>.find_previous_siblings()：在前序平行节点中搜索，返回列表类型\n",
    "- <>.find_previous_sibling()：在前序平行节点中返回一个结果\n",
    "\n",
    "区别：检索区域和返回结果不同\n",
    "\n",
    "具体请参阅文档 https://beautifulsoup.readthedocs.io/zh_CN/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise：**\n",
    "\n",
    "1. 请提取页面中的股票代票、股票名称、公告日期这三项信息，并将数据存放在dataframe中。\n",
    "2. 请提取出下述url页面中红色的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>代码</th>\n",
       "      <th>名称</th>\n",
       "      <th>日期</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300923</td>\n",
       "      <td>研奥股份</td>\n",
       "      <td>2020-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605377</td>\n",
       "      <td>华旺科技</td>\n",
       "      <td>2020-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>605179</td>\n",
       "      <td>一鸣食品</td>\n",
       "      <td>2020-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003028</td>\n",
       "      <td>振邦智能</td>\n",
       "      <td>2020-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003029</td>\n",
       "      <td>吉大正元</td>\n",
       "      <td>2020-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>688658</td>\n",
       "      <td>悦康药业</td>\n",
       "      <td>2020-12-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>688678</td>\n",
       "      <td>福立旺</td>\n",
       "      <td>2020-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>688698</td>\n",
       "      <td>伟创电气</td>\n",
       "      <td>2020-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>605186</td>\n",
       "      <td>健麾信息</td>\n",
       "      <td>2020-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>003026</td>\n",
       "      <td>中晶科技</td>\n",
       "      <td>2020-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>688063</td>\n",
       "      <td>派能科技</td>\n",
       "      <td>2020-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>605500</td>\n",
       "      <td>森林包装</td>\n",
       "      <td>2020-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>600659</td>\n",
       "      <td>*ST花雕</td>\n",
       "      <td>2020-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>003027</td>\n",
       "      <td>同兴环保</td>\n",
       "      <td>2020-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>605266</td>\n",
       "      <td>健之佳</td>\n",
       "      <td>2020-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>688699</td>\n",
       "      <td>明微电子</td>\n",
       "      <td>2020-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>002477</td>\n",
       "      <td>雏鹰退</td>\n",
       "      <td>2020-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>688608</td>\n",
       "      <td>恒玄科技</td>\n",
       "      <td>2020-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>688679</td>\n",
       "      <td>通源环境</td>\n",
       "      <td>2020-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>688560</td>\n",
       "      <td>明冠新材</td>\n",
       "      <td>2020-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>605299</td>\n",
       "      <td>舒华体育</td>\n",
       "      <td>2020-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>003025</td>\n",
       "      <td>思进智能</td>\n",
       "      <td>2020-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>605151</td>\n",
       "      <td>西上海</td>\n",
       "      <td>2020-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>003023</td>\n",
       "      <td>彩虹集团</td>\n",
       "      <td>2020-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>300919</td>\n",
       "      <td>中伟股份</td>\n",
       "      <td>2020-11-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        代码     名称          日期\n",
       "0   300923   研奥股份  2020-12-09\n",
       "1   605377   华旺科技  2020-12-09\n",
       "2   605179   一鸣食品  2020-12-08\n",
       "3   003028   振邦智能  2020-12-07\n",
       "4   003029   吉大正元  2020-12-07\n",
       "5   688658   悦康药业  2020-12-04\n",
       "6   688678    福立旺  2020-12-03\n",
       "7   688698   伟创电气  2020-12-02\n",
       "8   605186   健麾信息  2020-12-01\n",
       "9   003026   中晶科技  2020-12-01\n",
       "10  688063   派能科技  2020-12-01\n",
       "11  605500   森林包装  2020-12-01\n",
       "12  600659  *ST花雕  2020-12-01\n",
       "13  003027   同兴环保  2020-11-30\n",
       "14  605266    健之佳  2020-11-30\n",
       "15  688699   明微电子  2020-11-30\n",
       "16  002477    雏鹰退  2020-11-30\n",
       "17  688608   恒玄科技  2020-11-25\n",
       "18  688679   通源环境  2020-11-25\n",
       "19  688560   明冠新材  2020-11-25\n",
       "20  605299   舒华体育  2020-11-24\n",
       "21  003025   思进智能  2020-11-24\n",
       "22  605151    西上海  2020-11-24\n",
       "23  003023   彩虹集团  2020-11-24\n",
       "24  300919   中伟股份  2020-11-23"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "## 使用一个网易的数据页面\n",
    "url=\"http://quotes.money.163.com/data/caibao/yjgl_ALL.html?reportdate=20200930&sort=publishdate&order=desc&page=0\"\n",
    "     \n",
    "def request_url(url):\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.109 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent} \n",
    "    \n",
    "    res = requests.get(url,headers=headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    return res.text\n",
    "\n",
    "#### 提取三项信息并将数据存入dataframe中\n",
    "\n",
    "soup = BeautifulSoup(request_url(url), 'html.parser')\n",
    "# len(soup.find_all(table))\n",
    "soup.table.find_all('a')\n",
    "text = [i.text for i in soup.table.find_all('a', href = re.compile('#11a01'))]\n",
    "code = text[::2]\n",
    "name = text[1::2]\n",
    "date = [i.strip() for i in soup.table.find_all(string=re.compile('2020'))]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({'代码':code, '名称':name, '日期': date})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-6.70%',\n",
       " '-12.71%',\n",
       " '-5.67%',\n",
       " '-20.05%',\n",
       " '-2.75%',\n",
       " '-6.39%',\n",
       " '-0.03',\n",
       " '-2,495.65万',\n",
       " '-2.20%',\n",
       " '-0.47',\n",
       " '-77.70%',\n",
       " '-14.80亿',\n",
       " '-11.03%',\n",
       " '-0.45%',\n",
       " '-9.27%',\n",
       " '-5.83%']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 提取页面中红色的数值\n",
    "[x.text for x in soup.find_all('td', class_='red')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 基于上述实例，尝试通过修改url = 'https://finance.sina.com.cn/roll/index.d.html?cid=56588&page=1'\n",
    "中的关键词 page，实现自动翻页爬取多条信息，并存入本地数据库中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise：**\n",
    "\n",
    "1. 下面是一个批量下载百度图片的例子，请通过修改num，下载更多的图片，将num作为循环变量的参数写入程序中。\n",
    "2. 当下载的图片很多时会发生各种错误，如何让程序稳定地运行？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "pathname = 'D:\\\\图片\\\\'\n",
    "if not os.path.exists(pathname):\n",
    "    os.makedirs(pathname)\n",
    "name = input('您要爬取的图片名称：')\n",
    "num = 1\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'}\n",
    "url = 'https://image.baidu.com/search/flip?tn=baiduimage&ie=utf-8&word='+name+'&pn='+str((num-1)*60)\n",
    "res = requests.get(url,headers=headers)\n",
    "# print(res.request.url)\n",
    "\n",
    "html = res.content.decode()\n",
    "addlist = re.findall('\"objURL\":\"(.*?)\",',html)\n",
    "# print(len(addlist))\n",
    "# print(addlist)\n",
    "\n",
    "j=0\n",
    "for i in addlist:\n",
    "    j = j +1\n",
    "    img = requests.get(i)\n",
    "    f = open(pathname+name+str(j)+'.jpg','ab')\n",
    "    print('---------正在下载第'+str(j)+'张图片----------')\n",
    "    f.write(img.content)\n",
    "    f.close()\n",
    "print('下载完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**延申：**\n",
    "\n",
    "1. 在之前的课程中，我们提到过另一种解析器lxml，它提供另一种方法XPath来提取网页中的数据，感兴趣的同学可参阅[click](https://www.cnblogs.com/cathycheng/p/11422685.html)\n",
    "\n",
    "2. 思考如何爬取动态网页。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结：\n",
    "\n",
    "1. 提取NavigableString的几种方法：text、strings、stripped_strings\n",
    "\n",
    "2. 遍历网页标签树：上行，下行，平行\n",
    "\n",
    "3. 利用find_all方法搜索标签树：对标签名称、属性、内容的搜索（正则表达式）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
