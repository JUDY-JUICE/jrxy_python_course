{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(request_url(url),'lxml')\n",
    "text=[i for i in soup.table.stripped_strings]\n",
    "col=text[:11]\n",
    "code=text[12::11]\n",
    "name=text[13::11]\n",
    "date=text[20::11]\n",
    "# print(columns)\n",
    "# print(len(code))\n",
    "# print(len(name))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame(np.array([code,name,date]).T, columns=[col[1], col[2], col[-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 因为class是python的保留关键字，若要匹配标签内class的属性，需要特殊的方法，有以下两种：\n",
    "#### 1. BeautifulSoup自带的特别关键字class_\n",
    "#### 2. 在attrs属性用字典的方式进行参数传递\n",
    "\n",
    "soup = BeautifulSoup(request_url(url), 'lxml')\n",
    "[i.string for i in soup.find_all(class_='red')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(request_url(url), 'html.parser')\n",
    "# len(soup.find_all(table))\n",
    "soup.table.find_all('a')\n",
    "text = [i.text for i in soup.table.find_all('a', href = re.compile('#11a01'))]\n",
    "code = text[::2]\n",
    "name = text[1::2]\n",
    "date = [i.strip() for i in soup.table.find_all(string=re.compile('2020'))]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({'代码':code, '名称':name, '日期': date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SINA 爬虫实例\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen,Request\n",
    "from urllib.error import URLError,HTTPError\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd  \n",
    "from sqlalchemy import create_engine \n",
    "\n",
    "conn_sql = 'mysql+mysqldb://root:{}@127.0.0.1:3306/news_online?charset=utf8'.format(input('Please input \"password\":'))\n",
    "conn = create_engine(conn_sql)  \n",
    "\n",
    "def html_download(url):\n",
    "     headers = {\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/53'\n",
    "            }\n",
    "     request = Request(url,headers = headers)\n",
    "     try:\n",
    "         html = urlopen(request).read().decode()\n",
    "     except HTTPError as e:\n",
    "         html = None\n",
    "         print('请求服务器出错：%s'%e.reason)\n",
    "         return None\n",
    "     except URLError as e:\n",
    "         html = None\n",
    "         print('请求网页出错：%s'%e.reason)\n",
    "         return None\n",
    "     return html\n",
    " \n",
    "def json2df(json_results):\n",
    "    res = pd.DataFrame.from_records(json_results)\n",
    "    tags = []\n",
    "    for r in res.iterrows():\n",
    "        try:\n",
    "            tags.append(r[-1]['tag'][0]['name'])\n",
    "        except:\n",
    "            tags.append('其他')\n",
    "    x = res.loc[:,['id','commentid','creator','rich_text','update_time','zhibo_id']]\n",
    "    x['tag'] = tags\n",
    "    return x\n",
    " \n",
    "def api_info_manager(page, zhibo_id = 152):\n",
    "    #http://zhibo.sina.com.cn/api/zhibo/feed?&page=1&page_size=100&zhibo_id=152\n",
    "    data = {\n",
    "            'page':page,\n",
    "            'page_size':100,\n",
    "            'zhibo_id':zhibo_id\n",
    "            }\n",
    "    dataformat = 'http://zhibo.sina.com.cn/api/zhibo/feed?' + urlencode(data)\n",
    "    response = html_download(dataformat)\n",
    "    return json.loads(response,encoding = 'utf-8')['result']['data']['feed']['list']\n",
    "    #json_results = json.dumps(json_results,ensure_ascii = False)\n",
    "    #print(json_results)\n",
    "\n",
    "        \n",
    "def save_to_sql(res):\n",
    "    try:\n",
    "        r = res.sort_values(by='id', ascending = True)\n",
    "        # You need a database named news_online \n",
    "        pd.io.sql.to_sql(r,'sina_fin_news', con=conn, schema='news_online', if_exists = 'append')\n",
    "    except Exception:\n",
    "        print('Fail')\n",
    "\n",
    "def update_sql(res):\n",
    "    try:\n",
    "        last_id = int(pd.read_sql_query('select id from sina_fin_news ORDER BY id desc LIMIT 1', conn).id)\n",
    "        in_list = res[res['id']>last_id]\n",
    "        new_l = in_list.sort_values(by='id', ascending = True)\n",
    "        pd.io.sql.to_sql(new_l,'sina_fin_news', con=conn, schema='news_online', if_exists = 'append')\n",
    "       \n",
    "        print('{} items has been update Successed on {}'.format(len(new_l), time.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    except Exception:\n",
    "        \n",
    "        print('Fail to update on {}'.format(time.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    \n",
    "def main(page):\n",
    "    json_res = api_info_manager(page)\n",
    "    res = json2df(json_res)\n",
    "    save_to_sql(res)\n",
    "\n",
    "def updating():\n",
    "    while True:\n",
    "        json_res = api_info_manager(1)\n",
    "        res = json2df(json_res)\n",
    "        update_sql(res)\n",
    "        time.sleep(3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_res = api_info_manager(1)\n",
    "res = json2df(json_res)\n",
    "res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 雪球 \n",
    "\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "import pandas as pd  \n",
    "from sqlalchemy import create_engine \n",
    "\n",
    "\n",
    "conn_sql = 'mysql+mysqldb://root:{}@127.0.0.1:3306/news_online?charset=utf8'.format(input('Please input password:'))\n",
    "conn = create_engine(conn_sql)  \n",
    "\n",
    "\n",
    "\n",
    "def getcookies():#获得雪球网的cookie\n",
    "    headers3 = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "           'Referer': 'https://xueqiu.com/today',\n",
    "           'Host': 'xueqiu.com',\n",
    "           }\n",
    "    r = requests.get(url = 'https://xueqiu.com/', headers=headers3)\n",
    "    if r.status_code == 200:\n",
    "        cookie = r.cookies.get_dict()\n",
    "        return cookie\n",
    "    return None\n",
    "\n",
    "def html_download(url, cookie):\n",
    "    headers3 = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "               'referer': 'https://xueqiu.com/',\n",
    "               'Host': 'xueqiu.com',\n",
    "               }\n",
    "    try:\n",
    "        request = requests.get(url, headers=headers3,cookies=cookie)\n",
    "        if request.status_code == 200:\n",
    "            return request.text\n",
    "        return None\n",
    "    except RequestException:\n",
    "        return None\n",
    "\n",
    "\n",
    "def api_info_manager(cookie):\n",
    "    data = {\n",
    "            'since_id': -1,\n",
    "            'max_id': -1,\n",
    "            'count': 20,\n",
    "            'category': 6\n",
    "            }\n",
    "    dataformat = 'https://xueqiu.com/v4/statuses/public_timeline_by_category.json?' + urlencode(data)\n",
    "    response = html_download(dataformat,cookie)\n",
    "    if not response: \n",
    "        # if not works, do it again\n",
    "        cookie = getcookies()\n",
    "        response = html_download(dataformat,cookie)\n",
    "    \n",
    "    info = json.loads(response, encoding='utf-8')['list']\n",
    "    x = pd.DataFrame(columns =['id','category','text','target','view_count','created_at'])\n",
    "    for i,info in enumerate(json.loads(response, encoding='utf-8')['list']):\n",
    "        x.loc[i,'id'] = info['id']\n",
    "        x.loc[i,'category'] = info['category']\n",
    "        x.loc[i,'text'] = json.loads(info['data'])['text']\n",
    "        x.loc[i,'target'] = json.loads(info['data'])['target']\n",
    "        x.loc[i,'view_count'] = json.loads(info['data'])['view_count']\n",
    "        x.loc[i,'created_at'] = datetime.datetime.fromtimestamp(int(json.loads(info['data'])['created_at']/1000)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return x        \n",
    "     \n",
    "\n",
    "def update_sql(res):\n",
    "    try:\n",
    "        last_id = int(pd.read_sql_query('select id from xueqiu_fin_news ORDER BY id desc LIMIT 1', conn).id)\n",
    "        in_list = res[res['id']>last_id]\n",
    "        new_l = in_list.sort_values(by='id', ascending = True)\n",
    "        pd.io.sql.to_sql(new_l,'xueqiu_fin_news', con=conn, schema='news_online', if_exists = 'append')\n",
    "        print('{} items has been update Successed on {}'.format(len(new_l), time.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    except Exception:\n",
    "        print('Fail to update on {}'.format(time.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    \n",
    "def main(page):\n",
    "    cookie = getcookies()\n",
    "    res = api_info_manager(cookie)\n",
    "    update_sql(res)\n",
    "\n",
    "def updating(cookie):  \n",
    "    while True:\n",
    "        res = api_info_manager(cookie)\n",
    "        update_sql(res)\n",
    "        time.sleep(3600)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    #cookie = getcookies()\n",
    "    #updating(cookie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie = getcookies()\n",
    "res = api_info_manager(cookie)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'}\n",
    "pathname = 'D:\\\\图片\\\\'\n",
    "if not os.path.exists(pathname):\n",
    "    os.makedirs(pathname)\n",
    "name = input('您要爬取的图片名称：')\n",
    "x = input('您要爬取图片数量?，输入1等于60张图片。')\n",
    "num = 0\n",
    "for i in range(int(x)):\n",
    "    url = 'https://image.baidu.com/search/flip?tn=baiduimage&ie=utf-8&word='+name+'&pn='+str(i*60)\n",
    "    res = requests.get(url,headers=headers)\n",
    "    print(i)\n",
    "    print(res.request.url)\n",
    "    htlm_1 = res.content.decode()\n",
    "    a = re.findall('\"objURL\":\"(.*?)\",',htlm_1)\n",
    "    print(len(a))\n",
    "    for b in a:\n",
    "        num = num +1\n",
    "        try:\n",
    "            img = requests.get(b)\n",
    "        except Exception as e:\n",
    "            print('第'+str(num)+'张图片无法下载------------')\n",
    "            print(str(e))\n",
    "            continue\n",
    "        f = open(pathname+name+str(num)+'.jpg','ab')\n",
    "        print('---------正在下载第'+str(num)+'张图片----------')\n",
    "        f.write(img.content)\n",
    "        f.close()\n",
    "print('下载完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "您要爬取的图片名称： 壁纸\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------正在下载第61张图片----------\n",
      "---------正在下载第62张图片----------\n",
      "---------正在下载第63张图片----------\n",
      "---------正在下载第64张图片----------\n",
      "---------正在下载第65张图片----------\n",
      "---------正在下载第66张图片----------\n",
      "---------正在下载第67张图片----------\n",
      "---------正在下载第68张图片----------\n",
      "---------正在下载第69张图片----------\n",
      "---------正在下载第70张图片----------\n",
      "---------正在下载第71张图片----------\n",
      "---------正在下载第72张图片----------\n",
      "---------正在下载第73张图片----------\n",
      "---------正在下载第74张图片----------\n",
      "---------正在下载第75张图片----------\n",
      "---------正在下载第76张图片----------\n",
      "---------正在下载第77张图片----------\n",
      "---------正在下载第78张图片----------\n",
      "---------正在下载第79张图片----------\n",
      "---------正在下载第80张图片----------\n",
      "---------正在下载第81张图片----------\n",
      "---------正在下载第82张图片----------\n",
      "---------正在下载第83张图片----------\n",
      "---------正在下载第84张图片----------\n",
      "---------正在下载第85张图片----------\n",
      "---------正在下载第86张图片----------\n",
      "---------正在下载第87张图片----------\n",
      "---------正在下载第88张图片----------\n",
      "---------正在下载第89张图片----------\n",
      "第90张图片无法下载------------\n",
      "('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))\n",
      "---------正在下载第91张图片----------\n",
      "---------正在下载第92张图片----------\n",
      "---------正在下载第93张图片----------\n",
      "---------正在下载第94张图片----------\n",
      "---------正在下载第95张图片----------\n",
      "第96张图片无法下载------------\n",
      "('Connection aborted.', OSError(0, 'Error'))\n",
      "---------正在下载第97张图片----------\n",
      "---------正在下载第98张图片----------\n",
      "---------正在下载第99张图片----------\n",
      "---------正在下载第100张图片----------\n",
      "---------正在下载第101张图片----------\n",
      "---------正在下载第102张图片----------\n",
      "---------正在下载第103张图片----------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "pathname = 'D:\\\\图片\\\\'\n",
    "if not os.path.exists(pathname):\n",
    "    os.makedirs(pathname)\n",
    "name = input('您要爬取的图片名称：')\n",
    "num = 2\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'}\n",
    "url = 'https://image.baidu.com/search/flip?tn=baiduimage&ie=utf-8&word='+name+'&pn='+str((num-1)*60)\n",
    "res = requests.get(url,headers=headers)\n",
    "# print(res.request.url)\n",
    "\n",
    "html = res.content.decode()\n",
    "addlist = re.findall('\"objURL\":\"(.*?)\",',html)\n",
    "# print(len(addlist))\n",
    "# print(addlist)\n",
    "\n",
    "j=(num-1)*60\n",
    "for i in addlist:\n",
    "    j = j +1\n",
    "    try:\n",
    "        img = requests.get(i)\n",
    "    except Exception as e:\n",
    "        print('第'+str(j)+'张图片无法下载------------')\n",
    "        print(str(e))\n",
    "        continue\n",
    "    f = open(pathname+name+str(j)+'.jpg','ab')\n",
    "    print('---------正在下载第'+str(j)+'张图片----------')\n",
    "    f.write(img.content)\n",
    "    f.close()\n",
    "print('下载完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
