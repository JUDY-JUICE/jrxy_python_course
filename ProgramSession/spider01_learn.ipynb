{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 什么是爬虫\n",
    "\n",
    "**爬虫：** 一段自动抓取互联网信息的程序，从互联网上抓取对于我们有价值的信息。\n",
    "\n",
    "> 通过编程向网络服务器请求数据（HTML表单），然后解析HTML，提取出自己想要的数据\n",
    "\n",
    "![](spider01.jpg)\n",
    "\n",
    "归纳为四大步：\n",
    "\n",
    "- 根据url获取HTML数据\n",
    "- 解析HTML，获取目标信息\n",
    "- 存储数据\n",
    "- 重复第一步\n",
    "\n",
    "这会涉及到数据库、网络服务器、HTTP协议、HTML、数据科学、网络安全、图像处理等非常多的内容。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 什么是HTML\n",
    "\n",
    "HTML是英文Hyper Text Mark-up Language **(超文本标记语言)** 的缩写，它规定了自己的语法规则，用来表示比\"文本\"更丰富的意义，比如图片，表格，链接等。浏览器理解HTML语言的语法，可以用来查看HTML文档。目前互联网上的绝大部分网页都是使用HTML编写的。\n",
    "\n",
    "总结一下，HTML是一种用于创建网页的标记语言，里面嵌入了文本、图像等数据，可以被浏览器读取，并渲染成我们看到的网页样子。因此我们需要先 **爬取HTML**，再 **解析数据**，因为数据藏在HTML里。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一个HTML文档的例子：\n",
    "\n",
    "HTML语法的本质是**给文本加上表明文本含义的标签(Tag)**，让用户（人或程序）能对文本得到更好的理解。HTML文档由嵌套的HTML元素构成，它们用HTML标签表示，HTML标签包含于尖括号中。在一般情况下，一个元素由一对标签表示：“开始标签”<...>与“结束标签”</...>。元素如果含有文本内容，就被放置在这些标签之间。\n",
    "\n",
    "\n",
    "```\n",
    "<html>\n",
    "   <head>\n",
    "     <meta charset=\"UTF-8\">\n",
    "     <title>页面标题</title>\n",
    "   </head>\n",
    "   <body>\n",
    "     欢迎访问<a href=\"http://www.baidu.com/\">百度</a>！\n",
    "     <h1>我的第一个标题</h1>\n",
    "     <p>我的第一个段落。</p>\n",
    "   </body>\n",
    "</html> \n",
    "\n",
    "所有的HTML文档都应该有一个<html>标签，<html>标签可以包含两个部分:<head>和<body>。\n",
    "\n",
    "<head>标签用于包含整个文档的一般信息，比如文档的标题（<title>标签用于包含标题），对整个文档的描述，文档的关键字等等。文档的具体内容就要放在<body>标签里了。\n",
    "\n",
    "<a>标签用于表示链接，在浏览器中查看HTML文档时，点击<a>标签括起来的内容时，通常会跳转到另一个页面。这个要跳转到的页面的地址由<a>标签的href属性指定。上面的<a href=\"http:///www.baidu.com/\">中，href属性的值就是http://www.baidu.com/。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常见的标签如下：\n",
    "\n",
    "```python\n",
    "<html>..</html>       表示 HTML 页面的根元素\n",
    "<head>..</head>       包含了文档的元（meta）数据，如 <meta charset=\"utf-8\"> 定义网页编码格式为 utf-8\n",
    "<title>..</title>     描述了文档的标题\n",
    "<body>..</body>       包含用户可见的页面内容\n",
    "<h1>..</h1>           一级标题\n",
    "<p>..</p>             段落\n",
    "<div>..</div>         框架\n",
    "<table>..</table>     表格\n",
    "<a href=\"\">..</a>     超链接\n",
    "<img src=\"\" alt=\"\" /> 图片\n",
    "```\n",
    "**CSS** 表示样式，在 CSS 中定义了外观。\n",
    "\n",
    "**JScript** 描述了网站中的各种功能，交互的内容和各种特效都在 JScript 中。\n",
    "\n",
    "HTML（超文本标记语言）是Web的最基本构建模块，它定义了Web内容的含义和结构。除HTML外，其他技术通常用于描述网页的外观/样式（CSS）或功能/行为（JavaScript）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 爬虫的合法性\n",
    "\n",
    "网络爬虫的尺寸：\n",
    "\n",
    "- 小规模，数据量小，爬取速度不敏感（Requests库） >90%\n",
    "- 中规模，数据量较大，爬取速度较敏感（Scrapy库）\n",
    "\n",
    "网络爬虫的问题：\n",
    "- 性能骚扰：Web服务器默认接受人类访问，网络爬虫将会为其带来巨大的资源开销。\n",
    "- 法律风险：数据产权归属、隐私泄露等。\n",
    "\n",
    "网络爬虫的限制：\n",
    "\n",
    "- 来源审查：判断User‐Agent进行限制。检查来访HTTP协议头的User‐Agent域，只响应浏览器或友好爬虫的访问\n",
    "- 发布公告：**Robots协议**。告知所有爬虫网站的爬取策略，要求爬虫遵守\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robots协议\n",
    "\n",
    "Robots Exclusion Standard，网络爬虫排除标准\n",
    "- 作用：网站告知网络爬虫哪些页面可以抓取，哪些不行\n",
    "- 形式：在网站根目录下的robots.txt文件\n",
    "\n",
    "对于没有设定 robots.txt 的网站，可以通过网络爬虫获取没有口令加密的数据，也就是该网站所有页面数据都可以爬取。如果网站有 robots.txt 文档，就要判断是否有禁止访客获取的数据。\n",
    "\n",
    "**案例：雪球的Robots协议**\n",
    "\n",
    "https://xueqiu.com/robots.txt\n",
    "\n",
    "Robots协议基本语法：\n",
    "- User‐agent: *\n",
    "- Disallow: /\n",
    "\n",
    "语法示例：\n",
    "\n",
    "User-agent: * 代表所有的网络爬虫来源（ * 表示通配符 ）\n",
    "\n",
    "Disallow: /ABC/ 这里定义是禁止爬取ABC目录下面的目录\n",
    "\n",
    "Disallow: /cgi-bin/* .htm 禁止访问/cgi-bin/目录下的所有以\".htm\"为后缀的URL(包含子目录）。\n",
    "\n",
    "Disallow: /*?* 禁止访问网站中所有包含问号 (?) 的网址\n",
    "\n",
    "Disallow: /.jpg$ 禁止抓取网页所有的.jpg格式的图片\n",
    "\n",
    "Disallow:/ab/adc.html 禁止爬取ab文件夹下面的adc.html文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 爬虫的基本原理\n",
    "网页请求的过程分为两个环节：\n",
    "- Request （请求）：每一个展示在用户面前的网页都必须经过这一步，也就是向服务器发送访问请求。\n",
    "- Response（响应）：服务器在接收到用户的请求后，会验证请求的有效性，然后向用户（客户端）发送响应的内容，客户端接收服务器响应的内容，将内容展示出来，就是我们所熟悉的网页。\n",
    "\n",
    "网页请求的方式：\n",
    "\n",
    ">GET：最常见的方式，一般用于获取或者查询资源信息，也是大多数网站使用的方式，响应速度快。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests 库：\n",
    "- 安装：pip install requests\n",
    "- 调用：import requests\n",
    "\n",
    "get方法：\n",
    "`r = requests.get(url, params=None, **kwargs)`\n",
    "```\n",
    "url : 拟获取页面的url链接\n",
    "params : url中的额外参数，字典或字节流格式，可选\n",
    "**kwargs: 12个控制访问的参数\n",
    "```\n",
    "\n",
    "Reponse对象的属性：\n",
    "```\n",
    "r.status_code          HTTP请求的返回状态，200表示成功，404表示失败\n",
    "r.text                 HTTP响应内容的字符串形式，即，url对应的页面内容\n",
    "r.encoding             从HTTP header中猜测的响应内容编码方式\n",
    "r.apparent_encoding    从内容中分析出的响应内容编码方式（备选编码方式）\n",
    "r.content              HTTP响应内容的二进制形式\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 使用 GET 方式抓取数据\n",
    "\n",
    "import requests        # 导入requests包\n",
    "url = 'https://xueqiu.com/'\n",
    "res = requests.get(url)        # Get方式获取网页数据\n",
    "\n",
    "print(res.status_code)\n",
    "print(res.text)\n",
    "\n",
    "#### 查看response对象中request请求的headers属性：res.request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置headers\n",
    "\n",
    "网络爬虫的限制：检查来访HTTP协议头的User‐Agent域，只响应浏览器或友好爬虫的访问。因此在请求网页爬取的时候，输出的text信息中会出现抱歉，无法访问等字眼，这就是禁止爬取，此时需要通过反爬机制去解决这个问题。设置headers是解决requests请求反爬的方法之一，相当于我们通过设置headers信息，模拟成是浏览器在访问网站 。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 使用 GET 方式抓取数据\n",
    "\n",
    "import requests        # 导入requests包\n",
    "url = 'https://xueqiu.com/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\n",
    "res = requests.get(url, headers = headers)        # Get方式获取网页数据（设置headers）\n",
    "res.encoding = 'utf-8'  # 保证中文的显示\n",
    "res.status_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 百度搜索引擎关键词提交\n",
    "\n",
    "百度的关键词接口：http://www.baidu.com/s?wd=keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "kw = {'wd':'壁纸'}\n",
    "headers = {'User-Agent':'Mozilla/5.0'}\n",
    "r = requests.get(\"http://www.baidu.com/s\",params=kw,headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.request.url)\n",
    "print(len(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络图片的爬取和存储\n",
    "\n",
    "http://image.ngchina.com.cn/2019/1023/20191023050955671.gif\n",
    "\n",
    "https://ns-strategy.cdn.bcebos.com/ns-strategy/upload/fc_big_pic/part-00142-1269.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://image.ngchina.com.cn/2019/1023/20191023050955671.gif'\n",
    "path = 'abc.gif'\n",
    "# path = url.split('/')[-1]\n",
    "r = requests.get(url)\n",
    "print(r.status_code)\n",
    "\n",
    "with open(path, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise：**\n",
    "\n",
    "1. 请大家访问如下链接（或是你感兴趣的网站），查看它们的网页源代码和robots协议\n",
    "\n",
    ">http://news.sina.com.cn\n",
    "\n",
    ">http://news.qq.com\n",
    "\n",
    ">http://www.stats.gov.cn/ （无robots协议）\n",
    "\n",
    "2. 请大家尝试在百度图片中搜索你感兴趣的内容，并将图片保存到本地电脑的C盘根目录下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup\n",
    "\n",
    "官网地址：https://www.crummy.com/software/BeautifulSoup/\n",
    "\n",
    "BeautifulSoup 是 python 的一个第三方库，最主要的功能是用来提取 xml 和 HTML 中的数据。官方解释如下：\n",
    "\n",
    ">Beautiful Soup 提供一些简单的、python 式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。 Beautiful Soup 自动将输入文档转换为 Unicode 编码，输出文档转换为 utf-8 编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup 就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。 Beautiful Soup 已成为和 lxml、html6lib 一样出色的 python 解释器，为用户灵活地提供不同的解析策略或强劲的速度。\n",
    "\n",
    "安装方法：\n",
    "``pip install bs4``\n",
    "\n",
    "BeautifulSoup 支持 Python 标准库中的 HTML 解析器，也支持一些第三方的解析器。如果我们不安装它，则 Python 会使用 Python 默认的解析器 html.parser。lxml 解析器更加强大，速度更快，推荐安装。\n",
    "\n",
    "安装方法：\n",
    "``pip install lxml`` \n",
    "\n",
    "**一般步骤：**\n",
    "1. 通过requests库获得html页面的内容\n",
    "2. 使用BeautifulSoup库对获得的html页面进行解析\n",
    "3. 使用BeautifulSoup以及正则表达式来进一步提取我们想要的关键信息\n",
    "4. 将信息组织并输出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建BeautifulSoup对象 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests        # 导入requests包\n",
    "url = 'https://xueqiu.com/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "# headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\n",
    "res = requests.get(url, headers = headers)        # Get方式获取网页数据（设置headers）\n",
    "res.encoding = 'utf-8'  # 保证中文的显示\n",
    "demo = res.text\n",
    "\n",
    "from bs4 import BeautifulSoup         # 从bs4中导入BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(demo, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "demo = open('html.html','r',encoding='utf-8').read()    # 打开本地的html文件（注意html文件所在的位置）\n",
    "soup = BeautifulSoup(demo, 'lxml')\n",
    "soup.prettify()    # .prettify()方法用于为标签及字符串增加换行符\n",
    "print(demo) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup 将复杂 HTML 文档转换成一个复杂的树形结构，每个节点都是 Python 对象。因此Beautiful Soup库是解析、遍历、维护“标签树”的功能库。\n",
    "\n",
    "``<title>个股点评_证券_新浪财经</title>``\n",
    "\n",
    "``<a href=\"https://finance.sina.com.cn/stock/zqgd/2020-11-22/doc-iiznezxs3063378.shtml\" target=\"_blank\">*ST欧浦或面临退市:因公司控股股东佛山市中基投资宣告破产</a>``\n",
    "\n",
    "上述是两个标签的示例，利用 Beautiful Soup可以非常方便的将标签中的信息提取出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup类的基本元素\n",
    "```\n",
    "Tag：标签，最基本的信息组织单元，分别用<>和</>标明开头和结尾\n",
    "Name：标签的名字，<p>…</p>的名字是'p'，格式：<tag>.name\n",
    "Attributes：标签的属性，字典形式组织，格式：<tag>.attrs\n",
    "NavigableString：标签内非属性字符串，<>…</>中字符串，格式：<tag>.string\n",
    "Comment：标签内字符串的注释部分，一种特殊的Comment类型\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tag的Name和Attributes：**\n",
    "\n",
    "如何获取标签，标签名称和标签属性？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(soup.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取title标签的名称\n",
    "soup.title.name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取a标签父标签的名称\n",
    "soup.a.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取a标签的属性，返回字典\n",
    "soup.a.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单独获取 a 标签 href 属性的属性值\n",
    "soup.a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 get 方法，传入属性的名称，二者等价\n",
    "soup.a.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tag的NavigableString：**\n",
    "\n",
    "如何获取标签内部的文字？使用 .string或 .text，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意以下两者的区别\n",
    "soup.a.parent.parent.string\n",
    "soup.a.parent.parent.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise：**\n",
    "\n",
    "1. 请大家获取百度首页的html内容，并使用beautifulsoup对其进行解析，将其中的图片保存到本地C盘的根目录下。\n",
    "(tips: 利用img标签获取图片)\n",
    "\n",
    "2. 思考如何批量获取标签内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结：\n",
    "\n",
    "1. HTML——超文本标记语言：标签（Tag）\n",
    "\n",
    "2. 爬虫的框架、Robots协议\n",
    "\n",
    "1. 利用requests.get 方法获得html内容：通过设置headers将爬虫伪装成浏览器访问进行反爬虫\n",
    "\n",
    "2. 利用BeautifulSoup将网页解析为标签树：提取单个标签（名称、属性、NavigableString）\n",
    "\n",
    "中文官方文档：https://beautifulsoup.readthedocs.io/zh_CN/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
